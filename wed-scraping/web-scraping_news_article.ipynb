{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "821523e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import time\n",
    "\n",
    "def scrape_npr_with_tags(url, num_articles=100):\n",
    "    # Set up the Selenium WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    for _ in range(num_articles // 10):\n",
    "        driver.find_element(By.XPATH, '//body').send_keys(Keys.END)\n",
    "        time.sleep(2)  # Allow time for content to load\n",
    "\n",
    "    # Wait for the page to fully load\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//article')))\n",
    "\n",
    "    # Get the HTML content after dynamic loading\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "    # Find the elements containing news articles\n",
    "    article_elements = soup.find_all('article')[:num_articles]\n",
    "    df = []\n",
    "    try:\n",
    "    # Extract and print the headlines, summaries, tags, and sections\n",
    "        for article in article_elements:\n",
    "            headline = article.find('h2', class_='title').get_text(strip=True) if article.find('h2') else \"N/A\"\n",
    "            summary = article.find('p', class_='teaser').get_text(strip=True) if article.find('p') else \"N/A\"\n",
    "            sections = article.find('h3', class_='slug').get_text(strip=True) if article.find('h3') else \"N/A\"\n",
    "\n",
    "            list1 = []\n",
    "            if headline != 'N/A':\n",
    "                list1.append(sections)\n",
    "                list1.append(headline)\n",
    "                list1.append(summary)\n",
    "                df.append(list1)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Text Classification Model\n",
    "def classify_news_articles(scraped_data):\n",
    "    if len(scraped_data) < 2:\n",
    "        print(\"Insufficient data for training and testing. Please scrape more articles.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Convert the list to a DataFrame for easier handling\n",
    "    scraped_df = pd.DataFrame(scraped_data, columns=['section', 'headline', 'summary'])\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        scraped_df['summary'],\n",
    "        scraped_df['section'],\n",
    "    \n",
    "        test_size=0.1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    if len(X_train) == 0 or len(X_test) == 0:\n",
    "        print(\"Insufficient data for training and testing. Please scrape more articles.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Continue with the rest of the classification process\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    classifier = MultinomialNB()\n",
    "    classifier.fit(X_train_tfidf, y_train)\n",
    "    y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    evaluation_report = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
    "\n",
    "    return evaluation_report, accuracy, report\n",
    "\n",
    "\n",
    "# Main script\n",
    "def main():\n",
    "    count=1\n",
    "    # URL for news website\n",
    "    section_list=['sections/news/','sections/national/','sections/world/','sections/climate/','sections/health/']\n",
    "    links_list=[] \n",
    "    for link1 in section_list:\n",
    "        url=f\"https://www.npr.org/{link1}\"\n",
    "        r= requests.get(url)\n",
    "        htmlcontent = r.text\n",
    "\n",
    "\n",
    "\n",
    "        soup = BeautifulSoup(htmlcontent, 'html.parser')\n",
    "\n",
    "\n",
    "        links = soup.find_all('a')\n",
    "\n",
    "\n",
    "        for link in links:\n",
    "            href = link.get('href')\n",
    "            url_link=f\"https://www.npr.org/{href}\"\n",
    "            if url_link not in links_list:\n",
    "                links_list.append(url_link)\n",
    "    links_list\n",
    "    for l in section_list:\n",
    "        npr_url = f\"https://www.npr.org/{l}\"\n",
    "\n",
    "        # Scrape news articles\n",
    "        scraped_data = scrape_npr_with_tags(npr_url, num_articles=100)\n",
    "\n",
    "        if scraped_data is not None:\n",
    "            # Store the scraped data in the project folder\n",
    "            scraped_df = pd.DataFrame(scraped_data, columns=['section', 'headline', 'summary'])\n",
    "            scraped_df.to_csv(f'scraped_data{count}.csv', index=False)\n",
    "            count+=1\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d40f4212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            section  \\\n",
      "0                          Business   \n",
      "1  Middle East crisis â€” explained   \n",
      "2            The Unmarked Graveyard   \n",
      "3                       Middle East   \n",
      "4  Middle East crisis â€” explained   \n",
      "\n",
      "                                            headline  \\\n",
      "0  OpenAI reinstates Sam Altman as its chief exec...   \n",
      "1  Qatar says Israel and Hamas agree to a cease-f...   \n",
      "2  He disappeared in 1995. His mother's search le...   \n",
      "3                   Middle East crisis â€” explained   \n",
      "4  Pause in war gives hope to families of Israeli...   \n",
      "\n",
      "                                             summary  \n",
      "0  November 22, 2023 â€¢The company, maker of the...  \n",
      "1  November 21, 2023 â€¢Qatar's foreign ministry ...  \n",
      "2  November 22, 2023 â€¢LaMont Dottin was a fresh...  \n",
      "3  November 22, 2023 â€¢The conflict between Isra...  \n",
      "4  November 22, 2023 â€¢Hamas and Israel agreed t...  \n",
      "                            section  \\\n",
      "0                          Business   \n",
      "1  Middle East crisis â€” explained   \n",
      "2            The Unmarked Graveyard   \n",
      "3                       Middle East   \n",
      "4  Middle East crisis â€” explained   \n",
      "\n",
      "                                            headline  \\\n",
      "0  OpenAI reinstates Sam Altman as its chief exec...   \n",
      "1  Qatar says Israel and Hamas agree to a cease-f...   \n",
      "2  He disappeared in 1995. His mother's search le...   \n",
      "3                   Middle East crisis â€” explained   \n",
      "4  Pause in war gives hope to families of Israeli...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  November 22, 2023 â€¢The company, maker of the...   \n",
      "1  November 21, 2023 â€¢Qatar's foreign ministry ...   \n",
      "2  November 22, 2023 â€¢LaMont Dottin was a fresh...   \n",
      "3  November 22, 2023 â€¢The conflict between Isra...   \n",
      "4  November 22, 2023 â€¢Hamas and Israel agreed t...   \n",
      "\n",
      "                                   processed_summary  \n",
      "0  novemb compani maker popular chatgpt chatbot s...  \n",
      "1  novemb foreign ministri announc humanitarian p...  \n",
      "2  novemb dottin freshman queen colleg vanish one...  \n",
      "3  novemb conflict israel palestinian group middl...  \n",
      "4  novemb israel agre paus fight releas least isr...  \n",
      "Accuracy: 0.56\n",
      "                                  precision    recall  f1-score   support\n",
      "\n",
      "                         Animals       0.00      0.00      0.00         1\n",
      "                            Asia       1.00      1.00      1.00         1\n",
      "            Book News & Features       0.00      0.00      0.00         1\n",
      "                        Business       1.00      0.50      0.67         2\n",
      "          Consider This from NPR       0.00      0.00      0.00         1\n",
      "                          Europe       0.00      0.00      0.00         1\n",
      "                            Food       1.00      1.00      1.00         1\n",
      "                  Goats and Soda       0.40      0.50      0.44         4\n",
      "                   Latin America       0.00      0.00      0.00         1\n",
      "Middle East crisis â€” explained       1.00      1.00      1.00         2\n",
      "                        Politics       1.00      1.00      1.00         1\n",
      "                      Short Wave       1.00      1.00      1.00         1\n",
      "             Shots - Health News       0.17      1.00      0.29         1\n",
      "\n",
      "                        accuracy                           0.56        18\n",
      "                       macro avg       0.51      0.54      0.49        18\n",
      "                    weighted avg       0.54      0.56      0.52        18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sanket\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\Sanket\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Sanket\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Sanket\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "file_path = 'scraped_data.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df = df.dropna()\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [ps.stem(word.lower()) for word in tokens if (word.isalpha() and word.lower() not in stop_words)]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['processed_summary'] = df['summary'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['processed_summary'], df['section'], test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# SVC model\n",
    "classifier = SVC(kernel='linear')  # You can experiment with different kernels\n",
    "\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predictions on the test set\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# model evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "# Display classification report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c11347",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
